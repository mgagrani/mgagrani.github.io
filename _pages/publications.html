<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/multimodal_spd-480.webp 480w,/assets/img/publication_preview/multimodal_spd-800.webp 800w,/assets/img/publication_preview/multimodal_spd-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/multimodal_spd.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="multimodal_spd.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gagrani2024speculative" class="col-sm-8"> <div class="title">On Speculative Decoding for Multimodal Large Language Models</div> <div class="author"> <em>Mukul Gagrani*</em>, Raghavv Goel*, Wonseok Jeon, Junyoung Park, Mingu Lee, and Christopher Lott </div> <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awareded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2404.08856" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Selected as spotlight paper in ELVM workshop at CVPR 2024</p> </div> <div class="abstract hidden"> <p>Inference with Multimodal Large Language Models (MLLMs) is slow due to their large-language-model backbone which suffers from memory bandwidth bottleneck and generates tokens auto-regressively. In this paper, we explore the application of speculative decoding to enhance the inference efficiency of MLLMs, specifically the LLaVA 7B model. We show that a language-only model can serve as a good draft model for speculative decoding with LLaVA 7B, bypassing the need for image tokens and their associated processing components from the draft model. Our experiments across three different tasks show that speculative decoding can achieve a memory-bound speedup of up to 2.37x using a 115M parameter language model that we trained from scratch. Additionally, we introduce a compact LLaVA draft model incorporating an image adapter, which shows marginal performance gains in image captioning while maintaining comparable results in other tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rsd-480.webp 480w,/assets/img/publication_preview/rsd-800.webp 800w,/assets/img/publication_preview/rsd-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/rsd.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rsd.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jeon2024recursive" class="col-sm-8"> <div class="title">Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement</div> <div class="author"> Wonseok Jeon, <em>Mukul Gagrani</em>, Raghavv Goel, Junyoung Park, Mingu Lee, and Christopher Lott </div> <div class="periodical"> <em>arXiv preprint arXiv:2402.14160</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2402.14160" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree’s entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the diversity of the tree. During RSD’s drafting, the tree is built by either Gumbel-Top-k trick that draws tokens without replacement in parallel or Stochastic Beam Search that samples sequences without replacement while early-truncating unlikely draft sequences and reducing the computational cost of LLM. We empirically evaluate RSD with Llama 2 and OPT models, showing that RSD outperforms the baseline methods, consistently for fixed draft sequence length and in most cases for fixed computational budgets at LLM.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="goel2024direct" class="col-sm-8"> <div class="title">Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs</div> <div class="author"> Raghavv Goel, <em>Mukul Gagrani</em>, Wonseok Jeon, Junyoung Park, Mingu Lee, and Christopher Lott </div> <div class="periodical"> <em>ICLR Workshop on Mathematical and Empirical Understanding of Foundation Models</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=126PpV2CoO" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openreview.net/pdf?id=126PpV2CoO" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional alignment procedure. For the finetuning step, we use instruction-response pairs generated by target model for distillation in plausible data distribution, and propose a new Total Variation Distance++ (TVD++) loss that incorporates variance reduction techniques inspired from the policy gradient method in reinforcement learning. Our empirical results show that Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3 block efficiency and 2.4 speed-up relative to autoregressive decoding on various tasks with no further task-specific fine-tuning.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dag_scheduling-480.webp 480w,/assets/img/publication_preview/dag_scheduling-800.webp 800w,/assets/img/publication_preview/dag_scheduling-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/dag_scheduling.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dag_scheduling.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jeon2022neural" class="col-sm-8"> <div class="title">Neural DAG scheduling via one-shot priority sampling</div> <div class="author"> Wonseok Jeon*, <em>Mukul Gagrani*</em>, Burak Bartan, Weiliang Will Zeng, Harris Teague, Piero Zappi, and Christopher Lott </div> <div class="periodical"> <em>ICLR</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=WL8FlAugqQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We consider the problem of scheduling operations/nodes, the dependency among which is characterized by a Directed Acyclic Graph (DAG). Due to its NP-hard nature, heuristic algorithms were traditionally used to acquire reasonably good solutions, and more recent works have proposed Machine Learning (ML) heuristics that can generalize to unseen graphs and outperform the non-ML heuristics. However, it is computationally costly to generate solutions using existing ML schedulers since they adopt the episodic reinforcement learning framework that necessitates multi-round neural network processing. We propose a novel ML scheduler that uses a one-shot neural network encoder to sample node priorities which are converted by list scheduling to the final schedules. Since the one-shot encoder can efficiently sample the priorities in parallel, our algorithm runs significantly faster than existing ML baselines and has comparable run time with the fast traditional heuristics. We empirically show that our algorithm generates better schedules than both non-neural and neural baselines across various real-world and synthetic scheduling tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/topoformer-480.webp 480w,/assets/img/publication_preview/topoformer-800.webp 800w,/assets/img/publication_preview/topoformer-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/topoformer.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="topoformer.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gagrani2022neural" class="col-sm-8"> <div class="title">Neural topological ordering for computation graphs</div> <div class="author"> <em>Mukul Gagrani*</em>, Corrado Rainone*, Yang Yang, Harris Teague, Wonseok Jeon, Herke Van Hoof, Will Zeng, Piero Zappi, Christopher Lott, and Roberto Bondesan </div> <div class="periodical"> <em>NeurIPS</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=EvtEGQmXe3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recent works on machine learning for combinatorial optimization have shown that learning based approaches can outperform heuristic methods in terms of speed and performance. In this paper, we consider the problem of finding an optimal topological order on a directed acyclic graph with focus on the memory minimization problem which arises in compilers. We propose an end-to-end machine learning based approach for topological ordering using an encoder-decoder framework. Our encoder is a novel attention based graph neural network architecture called Topoformer which uses different topological transforms of a DAG for message passing. The node embeddings produced by the encoder are converted into node priorities which are used by the decoder to generate a probability distribution over topological orders. We train our model on a dataset of synthetically generated graphs called layered graphs. We show that our model outperforms, or is on-par, with several topological ordering baselines while being significantly faster on synthetic graphs with up to 2k nodes. We also train and test our model on a set of real-world computation graphs, showing performance improvements.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gagrani2022modified" class="col-sm-8"> <div class="title">A modified Thompson sampling-based learning algorithm for unknown linear systems</div> <div class="author"> <em>Mukul Gagrani</em>, Sagar Sudhakara, Aditya Mahajan, Ashutosh Nayyar, and Yi Ouyang </div> <div class="periodical"> <em>In 2022 IEEE 61st Conference on Decision and Control (CDC)</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2108.08502" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gagrani2021thompson" class="col-sm-8"> <div class="title">Thompson sampling for linear quadratic mean-field teams</div> <div class="author"> <em>Mukul Gagrani</em>, Sagar Sudhakara, Aditya Mahajan, Ashutosh Nayyar, and Yi Ouyang </div> <div class="periodical"> <em>In 2021 60th IEEE Conference on Decision and Control (CDC)</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2011.04686" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gagrani2021relaxed" class="col-sm-8"> <div class="title">A relaxed technical assumption for posterior sampling-based reinforcement learning for control of unknown linear systems</div> <div class="author"> <em>Mukul Gagrani</em>, Sagar Sudhakara, Aditya Mahajan, Ashutosh Nayyar, and Yi Ouyang </div> <div class="periodical"> <em>arXiv preprint arXiv:2108.08502</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="asghari2020regret" class="col-sm-8"> <div class="title">Regret analysis for learning in a multi-agent linear-quadratic control problem</div> <div class="author"> Seyed Mohammad Asghari, <em>Mukul Gagrani</em>, and Ashutosh Nayyar </div> <div class="periodical"> <em>In 2020 American Control Conference (ACC)</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://par.nsf.gov/servlets/purl/10213782" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gagrani2020weakly" class="col-sm-8"> <div class="title">Weakly coupled constrained Markov decision processes in Borel spaces</div> <div class="author"> <em>Mukul Gagrani</em>, and Ashutosh Nayyar </div> <div class="periodical"> <em>In 2020 American Control Conference (ACC)</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://par.nsf.gov/servlets/purl/10213781" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gagrani2020worst" class="col-sm-8"> <div class="title">Worst-case guarantees for remote estimation of an uncertain source</div> <div class="author"> <em>Mukul Gagrani</em>, Yi Ouyang, Mohammad Rasouli, and Ashutosh Nayyar </div> <div class="periodical"> <em>IEEE Transactions on Automatic Control</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/1902.03339" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="vasconcelos2020optimal" class="col-sm-8"> <div class="title">Optimal scheduling strategy for networked estimation with energy harvesting</div> <div class="author"> Marcos M Vasconcelos, <em>Mukul Gagrani</em>, Ashutosh Nayyar, and Urbashi Mitra </div> <div class="periodical"> <em>IEEE Transactions on Control of Network Systems</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ieeexplore.ieee.org/abstract/document/9099382" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="ouyang2019posterior" class="col-sm-8"> <div class="title">Posterior sampling-based reinforcement learning for control of unknown linear systems</div> <div class="author"> Yi Ouyang, <em>Mukul Gagrani</em>, and Rahul Jain </div> <div class="periodical"> <em>IEEE Transactions on Automatic Control</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/8884712" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We propose a posterior sampling-based learning algorithm for the linear quadratic (LQ) control problem with unknown system parameters. The algorithm is called posterior sampling-based reinforcement learning for LQ regulator (PSRL-LQ) where two stopping criteria determine the lengths of the dynamic episodes in posterior sampling. The first stopping criterion controls the growth rate of episode length. The second stopping criterion is triggered when the determinant of the sample covariance matrix is less than half of the previous value. We show under some conditions on the prior distribution that the expected (Bayesian) regret of PSRL-LQ accumulated up to time T is bounded by O (T−−√). Here, O (⋅) hides constants and logarithmic factors. Numerical simulations are provided to illustrate the performance of PSRL-LQ.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gagrani2018thompson" class="col-sm-8"> <div class="title">Thompson sampling for some decentralized control problems</div> <div class="author"> <em>Mukul Gagrani</em>, and Ashutosh Nayyar </div> <div class="periodical"> <em>In 2018 IEEE Conference on Decision and Control (CDC)</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ieeexplore.ieee.org/abstract/document/8619423" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gagrani2018scheduling" class="col-sm-8"> <div class="title">Scheduling and estimation strategies in a sequential networked estimation problem</div> <div class="author"> <em>Mukul Gagrani</em>, Marcos M Vasconcelos, and Ashutosh Nayyar </div> <div class="periodical"> <em>In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ieeexplore.ieee.org/abstract/document/8636085" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="ouyang2017learning" class="col-sm-8"> <div class="title">Learning-based control of unknown linear systems with thompson sampling</div> <div class="author"> Yi Ouyang, <em>Mukul Gagrani</em>, and Rahul Jain </div> <div class="periodical"> <em>arXiv preprint arXiv:1709.04047</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/1709.04047" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tsde_regret-480.webp 480w,/assets/img/publication_preview/tsde_regret-800.webp 800w,/assets/img/publication_preview/tsde_regret-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/tsde_regret.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tsde_regret.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ouyang2017learninh" class="col-sm-8"> <div class="title">Learning unknown markov decision processes: A thompson sampling approach</div> <div class="author"> Yi Ouyang, <em>Mukul Gagrani</em>, Ashutosh Nayyar, and Rahul Jain </div> <div class="periodical"> <em>NeurIPS</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/51ef186e18dc00c2d31982567235c559-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/51ef186e18dc00c2d31982567235c559-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We consider the problem of learning an unknown Markov Decision Process (MDP) that is weakly communicating in the infinite horizon setting. We propose a Thompson Sampling-based reinforcement learning algorithm with dynamic episodes (TSDE). At the beginning of each episode, the algorithm generates a sample from the posterior distribution over the unknown model parameters. It then follows the optimal stationary policy for the sampled model for the rest of the episode. The duration of each episode is dynamically determined by two stopping criteria. The first stopping criterion controls the growth rate of episode length. The second stopping criterion happens when the number of visits to any state-action pair is doubled. We establish bounds on expected regret under a Bayesian setting, where and are the sizes of the state and action spaces, is time, and is the bound of the span. This regret bound matches the best available bound for weakly communicating MDPs. Numerical results show it to perform better than existing algorithms for infinite horizon MDPs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="ouyang2017control" class="col-sm-8"> <div class="title">Control of unknown linear systems with thompson sampling</div> <div class="author"> Yi Ouyang, <em>Mukul Gagrani</em>, and Rahul Jain </div> <div class="periodical"> <em>In 2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</em> , 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ieeexplore.ieee.org/abstract/document/8262873/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gagrani2017decentralized" class="col-sm-8"> <div class="title">Decentralized minimax control problems with partial history sharing</div> <div class="author"> <em>Mukul Gagrani</em>, and Ashutosh Nayyar </div> <div class="periodical"> <em>In 2017 American Control Conference (ACC)</em> , 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ieeexplore.ieee.org/abstract/document/7963468/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gagrani2016centralized" class="col-sm-8"> <div class="title">Centralized minimax control</div> <div class="author"> <em>Mukul Gagrani</em>, and Ashutosh Nayyar </div> <div class="periodical"> 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ceng.usc.edu/techreports/2016/Nayyar%20CENG-2016-02.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gagrani2014transmit" class="col-sm-8"> <div class="title">Transmit and receive antenna pairing in MIMO relay networks</div> <div class="author"> <em>Mukul Gagrani</em>, and Ajit Kumar Chaturvedi </div> <div class="periodical"> <em>IEEE Communications Letters</em>, 2014 </div> <div class="periodical"> </div> <div class="links"> <a href="https://home.iitk.ac.in/%C2%A0akc/pdfs/Mukul_14.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2011</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gagrani2011noise" class="col-sm-8"> <div class="title">On noise-enhanced distributed inference in the presence of Byzantines</div> <div class="author"> <em>Mukul Gagrani</em>, Pranay Sharma, Satish Iyengar, V Sriram Siddhardh Nadendla, Aditya Vempaty, Hao Chen, and Pramod K Varshney </div> <div class="periodical"> <em>In 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</em> , 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="https://surface.syr.edu/cgi/viewcontent.cgi?article=1219&amp;context=eecs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li></ol> </div> </body></html>