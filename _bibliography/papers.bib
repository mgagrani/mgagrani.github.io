### LLM papers ####
@article{fastforward,
title={Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity},
author={Gautam, Aayush and Gagrani, Mukul and Park, Junyoung and Lee, Mingu and Lott, Christopher and Reddy, Narasimha},
journal={arXiv},
year={2026},
area={Efficient LLMs},
selected=true,
pdf={https://arxiv.org/pdf/2602.00397},
abstract={The prefill stage of large language model (LLM) inference is a key computational bottleneck for long-context
workloads. At short-to-moderate context lengths (1K–16K tokens), Feed-Forward Networks (FFNs) dominate this
cost, accounting for most of the total FLOPs. Existing FFN sparsification methods, designed for autoregressive
decoding, fail to exploit the prefill stage’s parallelism and often degrade accuracy. To address this, we introduce
FastForward, a predictive sparsity framework that accelerates LLM prefill through block-wise, context-aware FFN
sparsity. FastForward combines (1) a lightweight expert predictor to select high-importance neurons per block,
(2) an error compensation network to correct sparsity-induced errors, and (3) a layer-wise sparsity scheduler to
allocate compute based on token-mixing importance. Across LLaMA and Qwen models up to 8B parameters,
FastForward delivers up to 1.45× compute-bound speedup at 50% FFN sparsity with < 6% accuracy loss compared
to the dense baseline on LongBench, substantially reducing Time-to-First-Token (TTFT) for efficient, long-context
LLM inference on constrained hardware.}
}


@article{vocabtrim,
  title={VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs},
  author={Goel, Raghavv and Agrawal, Sudhanshu and Gagrani, Mukul and Park, Junyoung and Zao, Yifan and Zhang, He and Liu, Tian and Yang, Yiping and Yuan, Xin and Lu, Jiuyuan and Lott, Christopher and Lee, Mingu},
  journal={ICML Workshop on Efficient Systems for Foundational Models},
  year={2025},
  area={Efficient LLMs},
  selected=true,
  abbr={ICML},
  pdf={https://arxiv.org/pdf/2506.22694},
  abstract={In this paper, we introduce a simple training-free technique to improve the performance of drafter-based speculative decoding (SpD) methods that incorporates language modeling head (LM head) during drafting process. A drafter-based speculative decoding leverages one or more smaller language models, a.k.a. drafters or draft models, to sample a draft sequence or tree consisting of multiple tokens, followed by verification by a base LLM, a target model, accepting a subset as its valid generation. As it is usually considered that the speculative decoding requires one-to-one mapping between vocabularies of the target model and the draft model, it has been natural to share the vocabulary between them, or even share the LM head as in EAGLE or Medusa. We first identify that this draft token sampling scheme inherently contains an unnecessary inference overhead in drafting, especially for some target LLMs with very large vocabularies. Then, we propose a simple technique, VocabTrim, to mitigate the drafting overhead to improve the generation speed in memory-bound environment. VocabTrim reconstructs the drafter LM head to contain only a limited set of tokens, selected by the most frequently sampled from the vocabulary of the target model. While limiting the vocabulary in drafting slightly degrades the acceptance rate, it significantly reduces the drafting latency in memory-bound process which is often the case on edge devices, resulting in higher memory-bound speed up (MBSU). We show that our method can boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically by 16% for Llama-3.2-3B-Instruct.},
  }

@article{caote,
  title={CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based Token Eviction},
  author={Goel, Raghavv and Park, Junyoung and Gagrani, Mukul and Jones, Dalton and Morse, Matthew and Langston, Harper and Lee, Mingu and Lott, Christopher},
  journal={Submitted to MLSys},
  year={2025},
  area={Efficient LLMs},
  selected=true,
  pdf={https://arxiv.org/pdf/2504.14051},
  abstract={While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value vector information on top of attention-based eviction scores. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process},
  }

@article{gagrani2024speculative,
  title={On Speculative Decoding for Multimodal Large Language Models},
  author={Gagrani*, Mukul and Goel*, Raghavv and Jeon, Wonseok and Park, Junyoung and Lee, Mingu and Lott, Christopher},
  journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year={2024},
  area={Efficient LLMs},
  selected=true,
  abbr={CVPR},
  award={Selected as spotlight paper in ELVM workshop at CVPR 2024},
  pdf={https://arxiv.org/pdf/2404.08856},
  preview={multimodal_spd.png},
  abstract={Inference with Multimodal Large Language Models (MLLMs) is slow due to their large-language-model backbone which suffers from memory bandwidth bottleneck and generates tokens auto-regressively. In this paper, we explore the application of speculative decoding to enhance the inference efficiency of MLLMs, specifically the LLaVA 7B model. We show that a language-only model can serve as a good draft model for speculative decoding with LLaVA 7B, bypassing the need for image tokens and their associated processing components from the draft model. Our experiments across three different tasks show that speculative decoding can achieve a memory-bound speedup of up to 2.37x using a 115M parameter language model that we trained from scratch. Additionally, we introduce a compact LLaVA draft model incorporating an image adapter, which shows marginal performance gains in image captioning while maintaining comparable results in other tasks.},
  }

@article{jeon2024recursive,
  title={Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement},
  author={Jeon, Wonseok and Gagrani, Mukul and Goel, Raghavv and Park, Junyoung and Lee, Mingu and Lott, Christopher},
  journal={arXiv preprint arXiv:2402.14160},
  year={2024},
  area={Efficient LLMs},
  selected=true,
  pdf={https://arxiv.org/pdf/2402.14160},
  preview={rsd.png},
  abstract={Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the diversity of the tree. During RSD's drafting, the tree is built by either Gumbel-Top-k trick that draws tokens without replacement in parallel or Stochastic Beam Search that samples sequences without replacement while early-truncating unlikely draft sequences and reducing the computational cost of LLM. We empirically evaluate RSD with Llama 2 and OPT models, showing that RSD outperforms the baseline methods, consistently for fixed draft sequence length and in most cases for fixed computational budgets at LLM.},
}

@article{goel2024direct,
  title={Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs},
  author={Goel, Raghavv and Gagrani, Mukul and Jeon, Wonseok and Park, Junyoung and Lee, Mingu and Lott, Christopher},
  journal={ICLR Workshop on Mathematical and Empirical Understanding of Foundation Models},
  year={2024},
  html={https://openreview.net/forum?id=126PpV2CoO},
  area={Efficient LLMs},
  pdf={https://openreview.net/pdf?id=126PpV2CoO},
  selected=true,
  abbr={ICLR},
  abstract={Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional alignment procedure. For the finetuning step, we use instruction-response pairs generated by target model for distillation in plausible data distribution, and propose a new Total Variation Distance++ (TVD++) loss that incorporates variance reduction techniques inspired from the policy gradient method in reinforcement learning. Our empirical results show that Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3 block efficiency and 2.4 speed-up relative to autoregressive decoding on various tasks with no further task-specific fine-tuning.},
}



### MLCO papers ####

@article{jeon2022neural,
  title={Neural DAG scheduling via one-shot priority sampling},
  author={Jeon*, Wonseok and Gagrani*, Mukul and Bartan, Burak and Zeng, Weiliang Will and Teague, Harris and Zappi, Piero and Lott, Christopher},
  journal={ICLR},
  year={2022},
  area={ML for Combinatorial Optimization},
  selected=true,
  abbr={ICLR},
  pdf={https://openreview.net/pdf?id=WL8FlAugqQ},
  abstract={We consider the problem of scheduling operations/nodes, the dependency among
which is characterized by a Directed Acyclic Graph (DAG). Due to its NP-hard
nature, heuristic algorithms were traditionally used to acquire reasonably good solutions, and more recent works have proposed Machine Learning (ML) heuristics that
can generalize to unseen graphs and outperform the non-ML heuristics. However,
it is computationally costly to generate solutions using existing ML schedulers
since they adopt the episodic reinforcement learning framework that necessitates
multi-round neural network processing. We propose a novel ML scheduler that
uses a one-shot neural network encoder to sample node priorities which are converted by list scheduling to the final schedules. Since the one-shot encoder can
efficiently sample the priorities in parallel, our algorithm runs significantly faster
than existing ML baselines and has comparable run time with the fast traditional
heuristics. We empirically show that our algorithm generates better schedules
than both non-neural and neural baselines across various real-world and synthetic
scheduling tasks.},
preview={dag_scheduling.png},
}

@article{gagrani2022neural,
  title={Neural topological ordering for computation graphs},
  author={Gagrani*, Mukul and Rainone*, Corrado and Yang, Yang and Teague, Harris and Jeon, Wonseok and Van Hoof, Herke and Zeng, Will and Zappi, Piero and Lott, Christopher and Bondesan, Roberto},
  journal={NeurIPS},
  year={2022},
  area={ML for Combinatorial Optimization},
  selected=true,
  abbr={NeurIPS},
  abstract={Recent works on machine learning for combinatorial optimization have shown that
learning based approaches can outperform heuristic methods in terms of speed and
performance. In this paper, we consider the problem of finding an optimal topological order on a directed acyclic graph with focus on the memory minimization
problem which arises in compilers. We propose an end-to-end machine learning
based approach for topological ordering using an encoder-decoder framework.
Our encoder is a novel attention based graph neural network architecture called
Topoformer which uses different topological transforms of a DAG for message
passing. The node embeddings produced by the encoder are converted into node
priorities which are used by the decoder to generate a probability distribution over
topological orders. We train our model on a dataset of synthetically generated
graphs called layered graphs. We show that our model outperforms, or is on-par,
with several topological ordering baselines while being significantly faster on synthetic graphs with up to 2k nodes. We also train and test our model on a set of
real-world computation graphs, showing performance improvements.},
  pdf={https://openreview.net/pdf?id=EvtEGQmXe3},
  preview={topoformer.png},
}


#### Online learning #####

@inproceedings{gagrani2022modified,
  title={A modified Thompson sampling-based learning algorithm for unknown linear systems},
  author={Gagrani, Mukul and Sudhakara, Sagar and Mahajan, Aditya and Nayyar, Ashutosh and Ouyang, Yi},
  booktitle={2022 IEEE 61st Conference on Decision and Control (CDC)},
  pages={6658--6665},
  year={2022},
  organization={IEEE},
  area={Online Learning},
  pdf={https://arxiv.org/pdf/2108.08502},
}

@inproceedings{gagrani2021thompson,
  title={Thompson sampling for linear quadratic mean-field teams},
  author={Gagrani, Mukul and Sudhakara, Sagar and Mahajan, Aditya and Nayyar, Ashutosh and Ouyang, Yi},
  booktitle={2021 60th IEEE Conference on Decision and Control (CDC)},
  pages={720--727},
  year={2021},
  organization={IEEE},
  area={Online Learning},
  pdf={https://arxiv.org/pdf/2011.04686},
}

@article{gagrani2021relaxed,
  title={A relaxed technical assumption for posterior sampling-based reinforcement learning for control of unknown linear systems},
  author={Gagrani, Mukul and Sudhakara, Sagar and Mahajan, Aditya and Nayyar, Ashutosh and Ouyang, Yi},
  journal={arXiv preprint arXiv:2108.08502},
  year={2021},
  area={Online Learning},
}


@inproceedings{asghari2020regret,
  title={Regret analysis for learning in a multi-agent linear-quadratic control problem},
  author={Asghari, Seyed Mohammad and Gagrani, Mukul and Nayyar, Ashutosh},
  booktitle={2020 American Control Conference (ACC)},
  pages={3926--3931},
  year={2020},
  organization={IEEE},
  area={Online Learning},
  pdf={https://par.nsf.gov/servlets/purl/10213782},
}

@article{ouyang2019posterior,
  title={Posterior sampling-based reinforcement learning for control of unknown linear systems},
  author={Ouyang, Yi and Gagrani, Mukul and Jain, Rahul},
  journal={IEEE Transactions on Automatic Control},
  volume={65},
  number={8},
  pages={3600--3607},
  year={2019},
  publisher={IEEE},
  area={Online Learning},
  selected=true,
  html={https://ieeexplore.ieee.org/abstract/document/8884712},
  abstract={We propose a posterior sampling-based learning algorithm for the linear quadratic (LQ) control problem with unknown system parameters. The algorithm is called posterior sampling-based reinforcement learning for LQ regulator (PSRL-LQ) where two stopping criteria determine the lengths of the dynamic episodes in posterior sampling. The first stopping criterion controls the growth rate of episode length. The second stopping criterion is triggered when the determinant of the sample covariance matrix is less than half of the previous value. We show under some conditions on the prior distribution that the expected (Bayesian) regret of PSRL-LQ accumulated up to time T is bounded by O~(T−−√). Here, O~(⋅) hides constants and logarithmic factors. Numerical simulations are provided to illustrate the performance of PSRL-LQ.},
}


@inproceedings{gagrani2018thompson,
  title={Thompson sampling for some decentralized control problems},
  author={Gagrani, Mukul and Nayyar, Ashutosh},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)},
  pages={1053--1058},
  year={2018},
  organization={IEEE},
  area={Online Learning},
  html={https://ieeexplore.ieee.org/abstract/document/8619423},
}

@article{ouyang2017learning,
  title={Learning-based control of unknown linear systems with thompson sampling},
  author={Ouyang, Yi and Gagrani, Mukul and Jain, Rahul},
  journal={arXiv preprint arXiv:1709.04047},
  year={2017},
  area={Online Learning},
  pdf={https://arxiv.org/pdf/1709.04047},
}

@article{ouyang2017learning,
  title={Learning unknown markov decision processes: A thompson sampling approach},
  author={Ouyang, Yi and Gagrani, Mukul and Nayyar, Ashutosh and Jain, Rahul},
  journal={NeurIPS},
  volume={30},
  year={2017},
  area={Online Learning},
  selected=true,
  pdf={https://proceedings.neurips.cc/paper_files/paper/2017/file/51ef186e18dc00c2d31982567235c559-Paper.pdf},
  abstract={We consider the problem of learning an unknown Markov Decision Process (MDP) that is weakly communicating in the infinite horizon setting. We propose a Thompson Sampling-based reinforcement learning algorithm with dynamic episodes (TSDE). At the beginning of each episode, the algorithm generates a sample from the posterior distribution over the unknown model parameters. It then follows the optimal stationary policy for the sampled model for the rest of the episode. The duration of each episode is dynamically determined by two stopping criteria. The first stopping criterion controls the growth rate of episode length. The second stopping criterion happens when the number of visits to any state-action pair is doubled. We establish  bounds on expected regret under a Bayesian setting, where  and  are the sizes of the state and action spaces,  is time, and  is the bound of the span. This regret bound matches the best available bound for weakly communicating MDPs. Numerical results show it to perform better than existing algorithms for infinite horizon MDPs.},
  preview={tsde_regret.png},
  html={https://proceedings.neurips.cc/paper_files/paper/2017/hash/51ef186e18dc00c2d31982567235c559-Abstract.html},
  abbr={NeurIPS}
}

@inproceedings{ouyang2017control,
  title={Control of unknown linear systems with thompson sampling},
  author={Ouyang, Yi and Gagrani, Mukul and Jain, Rahul},
  booktitle={2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={1198--1205},
  year={2017},
  organization={IEEE},
  area={Online Learning},
  html={https://ieeexplore.ieee.org/abstract/document/8262873/},
}





### Control and communications ####



@inproceedings{gagrani2020weakly,
  title={Weakly coupled constrained Markov decision processes in Borel spaces},
  author={Gagrani, Mukul and Nayyar, Ashutosh},
  booktitle={2020 American Control Conference (ACC)},
  pages={2790--2795},
  year={2020},
  organization={IEEE},
  area={Stochastic Control and Communications},
  pdf={https://par.nsf.gov/servlets/purl/10213781},
}

@article{gagrani2020worst,
  title={Worst-case guarantees for remote estimation of an uncertain source},
  author={Gagrani, Mukul and Ouyang, Yi and Rasouli, Mohammad and Nayyar, Ashutosh},
  journal={IEEE Transactions on Automatic Control},
  volume={66},
  number={4},
  pages={1794--1801},
  year={2020},
  publisher={IEEE},
  area={Stochastic Control and Communications},
  pdf={https://arxiv.org/pdf/1902.03339},
}

@article{vasconcelos2020optimal,
  title={Optimal scheduling strategy for networked estimation with energy harvesting},
  author={Vasconcelos, Marcos M and Gagrani, Mukul and Nayyar, Ashutosh and Mitra, Urbashi},
  journal={IEEE Transactions on Control of Network Systems},
  volume={7},
  number={4},
  pages={1723--1735},
  year={2020},
  publisher={IEEE},
  area={Stochastic Control and Communications},
  html={https://ieeexplore.ieee.org/abstract/document/9099382},
}

@inproceedings{gagrani2018scheduling,
  title={Scheduling and estimation strategies in a sequential networked estimation problem},
  author={Gagrani, Mukul and Vasconcelos, Marcos M and Nayyar, Ashutosh},
  booktitle={2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={871--878},
  year={2018},
  organization={IEEE},
  area={Stochastic Control and Communications},
  html={https://ieeexplore.ieee.org/abstract/document/8636085},

}


@inproceedings{gagrani2017decentralized,
  title={Decentralized minimax control problems with partial history sharing},
  author={Gagrani, Mukul and Nayyar, Ashutosh},
  booktitle={2017 American Control Conference (ACC)},
  pages={3373--3379},
  year={2017},
  organization={IEEE},
  area={Stochastic Control and Communications},
  html={https://ieeexplore.ieee.org/abstract/document/7963468/},
}

@techreport{gagrani2016centralized,
  title={Centralized minimax control},
  author={Gagrani, Mukul and Nayyar, Ashutosh},
  year={2016},
  institution={Technical report CENG-2016-02, Sep 2016, http://ceng. usc. edu/techreports~…},
  area={Stochastic Control and Communications},
  pdf={https://ceng.usc.edu/techreports/2016/Nayyar%20CENG-2016-02.pdf},
}

@article{gagrani2014transmit,
  title={Transmit and receive antenna pairing in MIMO relay networks},
  author={Gagrani, Mukul and Chaturvedi, Ajit Kumar},
  journal={IEEE Communications Letters},
  volume={18},
  number={11},
  pages={2043--2046},
  year={2014},
  publisher={IEEE},
  area={Stochastic Control and Communications},
  pdf={https://home.iitk.ac.in/~akc/pdfs/Mukul_14.pdf},
}

@inproceedings{gagrani2011noise,
  title={On noise-enhanced distributed inference in the presence of Byzantines},
  author={Gagrani, Mukul and Sharma, Pranay and Iyengar, Satish and Nadendla, V Sriram Siddhardh and Vempaty, Aditya and Chen, Hao and Varshney, Pramod K},
  booktitle={2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={1222--1229},
  year={2011},
  organization={IEEE},
  area={Stochastic Control and Communications},
  pdf={https://surface.syr.edu/cgi/viewcontent.cgi?article=1219&context=eecs},
}
